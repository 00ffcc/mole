dim: 512
n_layers: 6
n_heads: 8
n_kv_heads: 4
vocab_size: 50304
hidden_dim: 2048
norm_eps: 1.0e-5
max_seq_len: 2048
rope_theta: 10000
dropout: 0.0
flash_attn: true

use_moe: false
num_experts_per_tok: 2
n_routed_experts: 4
n_shared_experts: true
scoring_func: softmax
aux_loss_alpha: 0.1
seq_aux: true
norm_topk_prob: true
arch: pre # pre, post, deepembed

# ple config
ple_dim: 256
# ple_layer_ids: [0, 1, 2, 3, 4, 5]
ple_layer_ids: []
optimizer_params:
  beta1: 0.9
  beta2: 0.999
  eps: 1.0e-8
  weight_decay: 0.0001
  grad_clip_max_norm: 1.0
offload_tok_embbedings: false
embedding_init_std: 1.0e-5
params_dtype: fp32 # embedding参数的类型 [fp32, bf16]

# Training config
is_profile: false
tokenizer_path: EleutherAI/pythia-70m-deduped
max_length: 2049
batch_size: 8
warmup_steps: 300
checkpoint_steps: 1000
max_lr: 1.0e-3
weight_decay: 0.0001
max_samples: 4770000
dataset_name_or_path: /NAS/wujunkang/guizhiyu/cache/datasets--pietrolesci--pile-deduped-pythia-preshuffled/snapshots/1b0c5082e922b45a3f937fa42934a91fc462e245

log_backend: swanlab # wandb or swanlab