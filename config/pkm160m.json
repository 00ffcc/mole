{
    "dim": 768,
    "n_layers": 12,
    "n_heads": 12,
    "n_kv_heads": 6,
    "vocab_size": 50304,
    "hidden_dim": 3072,
    "multiple_of": 64,
    "norm_eps": 1e-5,
    "max_seq_len": 2048,
    "rope_theta": 10000,
    "dropout": 0.0,
    "flash_attn": true,
    "norm_type": "pre",
    "@MoE": "MoE config",
    "use_moe": false,
    "num_experts_per_tok": 2,
    "n_routed_experts": 4,
    "n_shared_experts": true,
    "scoring_func": "softmax",
    "aux_loss_alpha": 0.1,
    "seq_aux": true,
    "norm_topk_prob": true,
    "@PKM": "PKM config",
    "use_lucidrains_pkm": false,
    "offload_tok_embbedings": true,
    "embedding_init_std": 1e-4,
    "pkm_k_dim": 256,
    "pkm_n_keys": 1024,
    "pkm_heads": 1,
    "pkm_knn": 32,
    "optimizer_params": {
        "beta1": 0.9,
        "beta2": 0.999,
        "eps": 1e-8,
        "weight_decay": 0.0001,
        "grad_clip_max_norm": 1.0
    },
    "pkm_layers": [[4,4], [8,8]],
    "@Training": "Training config",
    "is_profile": false,
    "tokenizer_path": "EleutherAI/pythia-70m-deduped",
    "max_length": 2049,
    "batch_size": 8,
    "warmup_steps": 3000,
    "checkpoint_steps": 1000,
    "max_lr": 6e-4,
    "weight_decay": 0.0001,
    "max_samples": 47700000
}
