dim: 128
n_layers: 6
n_heads: 4
n_kv_heads: 2
vocab_size: 50304
hidden_dim: 512
norm_eps: 1.0e-5
max_seq_len: 2048
rope_theta: 10000
dropout: 0.0
flash_attn: true

use_moe: false
num_experts_per_tok: 2
n_routed_experts: 4
n_shared_experts: true
scoring_func: softmax
aux_loss_alpha: 0.1
seq_aux: true
norm_topk_prob: true
norm_type: pre

# ple config
ple_dim: 64
# ple_layer_ids: [0, 1, 2, 3, 4, 5]
ple_layer_ids: []
optimizer_params:
  beta1: 0.9
  beta2: 0.999
  eps: 1.0e-8
  weight_decay: 0.0001
offload_tok_embbedings: true
embedding_init_std: 1.0e-5

# Training config
is_profile: false
tokenizer_path: EleutherAI/pythia-70m-deduped
max_length: 2049
batch_size: 8
warmup_steps: 300
checkpoint_steps: 1000
max_lr: 1.0e-3
weight_decay: 0.0001
max_samples: 4770000

