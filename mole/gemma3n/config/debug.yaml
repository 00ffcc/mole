model:
  arch: ple
  activation_sparsity_pattern: null
  altup_active_idx: 0
  altup_coef_clip: 120.0
  altup_correct_scale: true
  altup_lr_multiplier: 1.0
  altup_num_inputs: 1
  attention_bias: false
  attention_dropout: 0.0
  final_logit_softcapping: 30.0
  head_dim: 128
  hidden_activation: gelu_pytorch_tanh
  hidden_size: 64
  hidden_size_per_layer_input: 128
  initializer_range: 0.001
  intermediate_size: 512
  laurel_rank: 64
  layer_types: null
  max_position_embeddings: 32768
  num_attention_heads: 4
  num_hidden_layers: 6
  num_key_value_heads: 4
  num_kv_shared_layers: 0
  query_pre_attn_scalar: 256
  rms_norm_eps: !!float 1e-06
  rope_local_base_freq: 10000.0
  rope_scaling: null
  rope_theta: 1000000.0
  sliding_window: 512
  torch_dtype: float32 # bfloat16
  use_cache: false
  vocab_size: 50304 # 262400
  vocab_size_per_layer_input: 50304 # 262144
  attn_implementation: sdpa # or eager? sdpa? flash_attention_2?

project: gemma3n
is_profile: false
is_timing: true
tokenizer_name_or_path: EleutherAI/pythia-70m-deduped # or /NAS/wujunkang/guizhiyu/mole/tokenizer/gemma3
max_length: 2049
batch_size_per_device: 12
warmup_steps: 3000
checkpoint_steps: 10000000000
max_lr: !!float 1e-3
weight_decay: 0.0001
max_steps: 1000000
dataset_name_or_path: /NAS/wujunkang/guizhiyu/datasets/pile
log_backend: swanlab # wandb or swanlab